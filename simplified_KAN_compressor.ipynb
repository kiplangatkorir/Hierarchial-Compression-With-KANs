{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcVqr19yQB+bHSjig4mT3F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiplangatkorir/Hierarchical-Compression-of-LLM-Weights-using-Kolmogorov-Arnold-Networks/blob/main/simplified_KAN_compressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project implements a memory-efficient compression technique for Large Language Models (LLMs) using Principal Component Analysis (PCA). The goal is to significantly reduce the model size while maintaining as much performance as possible.\n",
        "\n"
      ],
      "metadata": {
        "id": "ywF-eZh48Lql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "In our tests using the MNIST dataset:\n",
        "\n",
        "Original Model Accuracy: 97.96%\n",
        "Compressed Model Accuracy: 93.38%\n",
        "Accuracy difference: 4.58 percentage points\n",
        "Compression ratio: 0.18 (82% reduction in model size)\n",
        "\n",
        "These results demonstrate a significant reduction in model size while maintaining strong performance."
      ],
      "metadata": {
        "id": "M6MGB0988frF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbKj8bKByS31",
        "outputId": "258ccc76-bbab-4ed0-87fd-6f80e44bc37a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "3NU_WmpiyhZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MemoryEfficientCompression:\n",
        "    def __init__(self, model, compression_ratio=0.5):\n",
        "        self.model = model\n",
        "        self.compression_ratio = compression_ratio\n",
        "        self.compressed_state = None\n",
        "        self.pca_models = {}\n",
        "\n",
        "    def compress(self):\n",
        "        compressed_state = {}\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.dim() > 1:\n",
        "                shape = param.shape\n",
        "                flattened = param.data.flatten().numpy()\n",
        "                # Calculate n_components based on the minimum of shape dimensions\n",
        "                n_components = max(1, int(min(shape) * self.compression_ratio))\n",
        "\n",
        "                pca = PCA(n_components=n_components)\n",
        "                compressed = pca.fit_transform(flattened.reshape(-1, shape[1]))\n",
        "\n",
        "                compressed_state[name] = {\n",
        "                    'compressed': torch.from_numpy(compressed).float(),\n",
        "                    'shape': shape,\n",
        "                    'mean': torch.from_numpy(pca.mean_).float(),\n",
        "                    'components': torch.from_numpy(pca.components_).float()\n",
        "                }\n",
        "                self.pca_models[name] = pca\n",
        "            else:\n",
        "                compressed_state[name] = param.data\n",
        "\n",
        "        self.compressed_state = compressed_state\n",
        "        return compressed_state\n",
        "\n",
        "    def decompress(self):\n",
        "        if self.compressed_state is None:\n",
        "            raise ValueError(\"Model hasn't been compressed yet.\")\n",
        "\n",
        "        decompressed_state = {}\n",
        "        for name, compressed_data in self.compressed_state.items():\n",
        "            if isinstance(compressed_data, dict):  # Compressed tensor\n",
        "                pca = self.pca_models[name]\n",
        "                decompressed = pca.inverse_transform(compressed_data['compressed'].numpy())\n",
        "                decompressed = torch.from_numpy(decompressed).float().view(compressed_data['shape'])\n",
        "                decompressed_state[name] = decompressed\n",
        "            else:  # Uncompressed tensor\n",
        "                decompressed_state[name] = compressed_data\n",
        "\n",
        "        return decompressed_state\n",
        "\n",
        "    def apply_compressed_weights(self):\n",
        "        decompressed_state = self.decompress()\n",
        "        with torch.no_grad():\n",
        "            for name, param in self.model.named_parameters():\n",
        "                param.copy_(decompressed_state[name])\n",
        "\n",
        "def compress_model(model, compression_ratio=0.5):\n",
        "    compressor = MemoryEfficientCompression(model, compression_ratio)\n",
        "    compressed_state = compressor.compress()\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    compressed_params = sum(c['compressed'].numel() for c in compressed_state.values() if isinstance(c, dict))\n",
        "    compressed_params += sum(c.numel() for c in compressed_state.values() if not isinstance(c, dict))\n",
        "\n",
        "    print(f\"Original parameters: {total_params}\")\n",
        "    print(f\"Compressed parameters: {compressed_params}\")\n",
        "    print(f\"Compression ratio: {compressed_params / total_params:.2f}\")\n",
        "\n",
        "    return compressor"
      ],
      "metadata": {
        "id": "HS2YbkOLykjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ")\n",
        "\n",
        "compressor = compress_model(model, compression_ratio=0.5)\n",
        "\n",
        "# To use the compressed model:\n",
        "compressor.apply_compressed_weights()\n",
        "print(\"Compression and decompression complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99Ih6qLjyyZi",
        "outputId": "332907f4-417a-4608-a078-788d3712a5e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original parameters: 235146\n",
            "Compressed parameters: 41404\n",
            "Compression ratio: 0.18\n",
            "Compression and decompression complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing Phase**"
      ],
      "metadata": {
        "id": "fuFKJfe21Dk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "bZH9aznr1BHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "def load_mnist_data():\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "qx5Ck3Fq1CYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, epochs=10, lr=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data.view(data.size(0), -1))\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                      f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "hgd9o6YX1Rkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data.view(data.size(0), -1))\n",
        "            test_loss += nn.functional.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f})')\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "bhJZdZLR1Wqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST data\n",
        "train_loader, test_loader = load_mnist_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiuST_wZ4pyO",
        "outputId": "408dd720-4449-4ba3-a44f-f61352446201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 56013234.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1868801.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 12859525.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2629472.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ")"
      ],
      "metadata": {
        "id": "b5rTdRYI1acx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the original model\n",
        "print(\"Training original model...\")\n",
        "model = train_model(model, train_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK95xnYq1fW5",
        "outputId": "e5453712-06af-4267-d0b4-c726ef5d2fa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training original model...\n",
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.302960\n",
            "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.124060\n",
            "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.255756\n",
            "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.412142\n",
            "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.163996\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.283970\n",
            "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.269279\n",
            "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.118528\n",
            "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.132483\n",
            "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.127954\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.065644\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.012657\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.029933\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.067347\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.093317\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.026376\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.123428\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.044560\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.082078\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.098903\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.028986\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.078320\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.277784\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.115411\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.021385\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.039684\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.051479\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.028004\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.055845\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.025668\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.020247\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.004831\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.008310\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.011148\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.045891\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.068124\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.018355\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.096674\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.034634\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.032446\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.005589\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.010803\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.032257\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005665\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.006786\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.231874\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.006719\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.026417\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.022058\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.236109\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.020095\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.119516\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.013676\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.009180\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.067954\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.008799\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.007859\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.068563\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.021083\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.008367\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.016504\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.096185\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.017065\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.012416\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.006703\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.047517\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.003708\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.008448\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.015368\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.004251\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.005839\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.001574\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.000183\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.020929\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.001818\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.034534\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.020932\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.005286\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.009292\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.003607\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.039583\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.001548\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.065100\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.005342\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.003649\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.010792\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.023139\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.024890\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.008881\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.001100\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.004811\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.001030\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.027483\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.003009\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.018463\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.006067\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.253015\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.001051\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.001919\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.118544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the original model\n",
        "print(\"\\nEvaluating original model...\")\n",
        "original_accuracy = evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmzGT_Dk166c",
        "outputId": "7fc79ad9-2e2d-4ef0-a7e1-175183752ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating original model...\n",
            "\n",
            "Test set: Average loss: 0.0913, Accuracy: 9796/10000 (0.98)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compress the model\n",
        "print(\"\\nCompressing model...\")\n",
        "compressor = compress_model(model, compression_ratio=0.5)\n",
        "compressor.apply_compressed_weights()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZziHfIE2AVJ",
        "outputId": "8d3042f9-8183-480b-c982-5bdb83b37c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Compressing model...\n",
            "Original parameters: 235146\n",
            "Compressed parameters: 41404\n",
            "Compression ratio: 0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the compressed model\n",
        "print(\"\\nEvaluating compressed model...\")\n",
        "compressed_accuracy = evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f619u4ID6P9l",
        "outputId": "ae8f2c45-0578-44a3-8b79-309fe096838d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating compressed model...\n",
            "\n",
            "Test set: Average loss: 0.2274, Accuracy: 9338/10000 (0.93)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nOriginal Model Accuracy: {original_accuracy:.4f}\")\n",
        "print(f\"Compressed Model Accuracy: {compressed_accuracy:.4f}\")\n",
        "print(f\"Accuracy difference: {original_accuracy - compressed_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV6v93Zu6USd",
        "outputId": "8f5a73c1-a7c4-48e3-de64-8325275fdbdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Model Accuracy: 0.9796\n",
            "Compressed Model Accuracy: 0.9338\n",
            "Accuracy difference: 0.0458\n"
          ]
        }
      ]
    }
  ]
}